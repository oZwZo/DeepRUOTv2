{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import anndata as ad\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from DeepRUOT.losses import OT_loss1\n",
    "from DeepRUOT.utils import (\n",
    "    generate_steps, load_and_merge_config,\n",
    "    SchrodingerBridgeConditionalFlowMatcher,\n",
    "    generate_state_trajectory, get_batch, get_batch_size\n",
    ")\n",
    "from DeepRUOT.train import train_un1_reduce, train_all\n",
    "from DeepRUOT.models import FNet, scoreNet2\n",
    "from DeepRUOT.constants import DATA_DIR, RES_DIR\n",
    "from DeepRUOT.exp import setup_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/lustre/home/2100011778/DeepRUOTv2_test_data/config/test_data.yaml'\n",
    "\n",
    "# Load and merge configuration\n",
    "config = load_and_merge_config(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, config['data']['file_path']))\n",
    "df = df.iloc[:, :config['data']['dim'] + 1]\n",
    "device = torch.device('cpu')\n",
    "exp_dir, logger = setup_exp(\n",
    "            RES_DIR, \n",
    "            config, \n",
    "            config['exp']['name']\n",
    "        )\n",
    "dim = config['data']['dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model']\n",
    "        \n",
    "f_net = FNet(\n",
    "    in_out_dim=model_config['in_out_dim'],\n",
    "    hidden_dim=model_config['hidden_dim'],\n",
    "    n_hiddens=model_config['n_hiddens'],\n",
    "    activation=model_config['activation']\n",
    ").to(device)\n",
    "\n",
    "sf2m_score_model = scoreNet2(\n",
    "    in_out_dim=model_config['in_out_dim'],\n",
    "    hidden_dim=model_config['score_hidden_dim'],\n",
    "    activation=model_config['activation']\n",
    ").float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_net.load_state_dict(torch.load(os.path.join(exp_dir, 'model_final'),map_location=torch.device('cpu')))\n",
    "f_net.to(device)\n",
    "sf2m_score_model.load_state_dict(torch.load(os.path.join(exp_dir, 'score_model_final'),map_location=torch.device('cpu')))\n",
    "sf2m_score_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dim reduction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import umap\n",
    "# umap_op = PCA(n_components=2)\n",
    "umap_op = umap.UMAP(n_components=2, random_state=42) # You may change UMAP to PCA or other dimension reduction methods\n",
    "xu = umap_op.fit_transform(df.iloc[:, 1:])  # Assuming df is your DataFrame\n",
    "joblib.dump(umap_op, os.path.join(exp_dir, 'dim_reduction.pkl'))  # Save the UMAP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import joblib\n",
    "# Assume the following are defined:\n",
    "# - df: DataFrame with columns 'samples', 'x1', 'x2', ..., 'xn'\n",
    "# - dim: Number of dimensions (n)\n",
    "# - device: torch.device (e.g., 'cpu' or 'cpu')\n",
    "# - sf2m_score_model: Your model function that takes t_tensor and data_tensor\n",
    "\n",
    "dim_reducer = joblib.load(os.path.join(exp_dir, 'dim_reduction.pkl')) # If no dim reducer is needed, set this to None\n",
    "\n",
    "# Step 1: Extract all time points and data points\n",
    "\n",
    "\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "#t_value = 2.0  \n",
    "t_tensor = torch.tensor(all_times).unsqueeze(1).float().to(device)\n",
    "\n",
    "# Step 2: Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "data_tensor.requires_grad_(True)  # Enable gradient tracking\n",
    "#t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "lnw0 = torch.log(torch.ones(data_tensor.shape[0], 1) / data_tensor.shape[0]).to(device)\n",
    "# Step 3: Compute log density values and gradients\n",
    "gradients = f_net.v_net(t_tensor, data_tensor) \n",
    "\n",
    "# Step 4: Move data to CPU for plotting\n",
    "data_np = data_tensor.detach().cpu().numpy()\n",
    "if dim_reducer is not None:\n",
    "    data_np_2d=dim_reducer.transform(data_np)\n",
    "else:\n",
    "    data_np_2d=data_np[:,:2]\n",
    "gradients_np = gradients.detach().cpu().numpy()\n",
    "data_end = data_np + gradients_np\n",
    "if dim_reducer is not None:\n",
    "    data_end_2d=dim_reducer.transform(data_end)\n",
    "else:\n",
    "    data_end_2d=data_end[:,:2]\n",
    "gradients_np=data_end_2d - data_np_2d\n",
    "\n",
    "gradients_np = gradients_np / np.linalg.norm(gradients_np, axis = 1, keepdims=True) * 5\n",
    "times_np = all_times\n",
    "data_np = data_np_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "# Assume your data is already defined\n",
    "# all_data: 50-dimensional data with shape (n_cells, 50)\n",
    "# gradients: 50-dimensional vector field with shape (n_cells, 50) \n",
    "# pca: Trained PCA or UMAP model for generating X_umap\n",
    "# X_umap: Pre-computed UMAP embeddings with shape (n_cells, 2)\n",
    "\n",
    "# Create AnnData object\n",
    "adata = anndata.AnnData(X=all_data)\n",
    "\n",
    "# Set 'Ms' layer to avoid KeyError: 'Ms'\n",
    "adata.layers['Ms'] = all_data  # Use original 50-dim data as state matrix\n",
    "# Optional: normalize and log transform (based on data needs)\n",
    "# sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "# sc.pp.log1p(adata)\n",
    "# adata.layers['Ms'] = adata.X.copy()\n",
    "\n",
    "# Set velocity vectors\n",
    "adata.layers['velocity'] = gradients.detach().cpu().numpy()  # Store velocity vectors in layers\n",
    "print(adata.layers['velocity'].shape)  # Confirm shape\n",
    "\n",
    "\n",
    "sc.pp.neighbors(adata, use_rep='X', n_neighbors=30, metric='euclidean', random_state=42)\n",
    "# 计算 UMAP\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "# Set pre-computed UMAP embeddings\n",
    "if dim_reducer is not None:\n",
    "    X_umap = dim_reducer.transform(all_data)  # Assume pca is trained dim reduction model\n",
    "else:\n",
    "    X_umap = all_data[:,:2]\n",
    "# adata.obsm['X_umap'] = X_umap\n",
    "print(adata.obsm['X_umap'].shape)  # Confirm UMAP embedding shape\n",
    "adata.obs['time'] = all_times\n",
    "if adata.layers['velocity'].shape[1] != 2:\n",
    "    # Calculate neighbor graph (required for velocity graph)\n",
    "    sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')  # Calculate neighbors based on high-dim data\n",
    "\n",
    "    # Calculate velocity graph\n",
    "    scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=16)  # Build velocity graph from high-dim velocity vectors\n",
    "\n",
    "    # Project velocities to UMAP space\n",
    "    scv.tl.velocity_embedding(adata, basis='umap', vkey='velocity')  # Project velocities to UMAP\n",
    "else:\n",
    "    adata.obsm['velocity_umap'] = adata.layers['velocity']\n",
    "\n",
    "# Plot\n",
    "\n",
    "adata.obs['time_categorical'] = pd.Categorical(adata.obs['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "meta_data = sc.read_h5ad('/lustre/home/2100011778/spatial_data/h5ad/stromal_mnn.h5ad')\n",
    "# meta_data = pd.read_csv('D:\\DeepRUOTv2\\\\test_data\\data_0625\\WO_FG_D10_D20_pro_HB_tumor_metadata.csv', sep=' ')\n",
    "# Split celltype by underscore and take everything except first word\n",
    "# meta_data['celltype'] = meta_data['celltype'].str.split('_').str[1:].str.join('_')\n",
    "print(meta_data.obs['cellType_new3'])\n",
    "adata.obs['celltype'] = meta_data.obs['cellType_new3'].values\n",
    "\n",
    "# Visualization settings\n",
    "scv.settings.set_figure_params('scvelo')  # Set scvelo plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.obsm['X_pca'] = df.iloc[:,1:].values\n",
    "#meta_data.layers['velocity'] = gradients.detach().cpu().numpy()\n",
    "# Calculate velocity graph\n",
    "#scv.tl.velocity_graph(meta_data, vkey='velocity', n_jobs=16)  # Build velocity graph from high-dim velocity vectors\n",
    "\n",
    "# Project velocities to UMAP space\n",
    "#scv.tl.velocity_embedding(meta_data, basis='umap', vkey='velocity')  # Project velocities to UMAP\n",
    "sc.pp.neighbors(meta_data, n_neighbors=30, use_rep='X_pca')\n",
    "sc.tl.paga(meta_data, groups='celltype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算PAGA图\n",
    "sc.tl.paga(meta_data, groups='celltype', use_rna_velocity=True)\n",
    "# scv.tl.paga(meta_data, groups='celltype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化PAGA + Velocity箭头\n",
    "sc.pl.paga(meta_data, color='celltype', show=False)\n",
    "# scv.pl.velocity_embedding_stream(adata, basis='umap', color='celltype', ax=plt.gca(), show=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.paga(\n",
    "  meta_data,\n",
    "  threshold = 0.15,\n",
    "  arrowsize = 10,\n",
    "  edge_width_scale = 0.5,\n",
    "  transitions = \"transitions_confidence\",\n",
    "  dashed_edges = \"connectivities\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 adata 已经包含PAGA结果和聚类信息（例如 'leiden'）\n",
    "# 如果没有UMAP嵌入，计算UMAP\n",
    "sc.tl.umap(meta_data, init_pos='paga')  # 使用PAGA初始化UMAP（可选）\n",
    "\n",
    "# 绘制UMAP图，按聚类着色\n",
    "sc.pl.umap(meta_data, color='celltype', title='UMAP colored by Leiden clusters', show=True)\n",
    "scv.pl.velocity_embedding_stream(meta_data, basis='umap', color='celltype', ax=plt.gca(), show=False)\n",
    "# # 可选：叠加PAGA路径到UMAP图\n",
    "# sc.pl.umap(adata, color='celltype', title='UMAP with PAGA paths', show=False)\n",
    "# sc.pl.paga(adata, plot=True, ax=plt.gca(), show=False)  # 叠加PAGA路径\n",
    "# plt.show()\n",
    "\n",
    "# 可选：按特定基因着色\n",
    "# sc.pl.umap(adata, color=['gene_name1', 'gene_name2'], title='UMAP colored by gene expression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PAGA初始化的UMAP\n",
    "sc.tl.umap(adata, init_pos='paga')\n",
    "\n",
    "# 可视化UMAP图，并可以同时展示PAGA的连接关系\n",
    "sc.pl.umap(adata,  legend_loc='on data', title='', frameon=False, legend_fontsize=10)\n",
    "sc.pl.paga_compare(adata, legend_fontsize=10, frameon=False, size=20, title='', legend_fontoutline=2, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.pl.velocity_embedding_stream(\n",
    "    adata,\n",
    "    basis='umap',\n",
    "    color='celltype',\n",
    "    figsize=(7, 5),\n",
    "    density=3,\n",
    "    title='Velocity Stream Plot',\n",
    "    legend_loc=None, #'right',\n",
    "    # Available palettes:\n",
    "    # - 'tab20': 20 distinct colors\n",
    "    # - 'viridis': Sequential colormap from dark blue to yellow\n",
    "    # - 'plasma': Sequential colormap from dark purple to yellow\n",
    "    # - 'inferno': Sequential colormap from black to yellow\n",
    "    # - 'magma': Sequential colormap from black to light pink\n",
    "    # - 'Set1', 'Set2', 'Set3': Qualitative colormaps with distinct colors\n",
    "    # - 'Paired': Qualitative colormap with paired colors\n",
    "    # - 'husl': Evenly spaced colors in HUSL space\n",
    "    palette='tab20',\n",
    "    save= exp_dir+'/velocity_stream_plot.svg'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "# Load dimension reducer\n",
    "dim_reducer = joblib.load(os.path.join(exp_dir, 'dim_reduction.pkl')) # If no dim reducer is needed, set this to None\n",
    "device = 'cpu'\n",
    "f_net = f_net.to(device)\n",
    "\n",
    "def plot_g_values(df, f_net, dim_reducer=None, device=device):\n",
    "    # Get all time points\n",
    "    time_points = df['samples'].unique()\n",
    "    \n",
    "    # Store data for each time point\n",
    "    data_by_time = {}\n",
    "    \n",
    "    # Calculate g_values for each time point\n",
    "    for time in time_points:\n",
    "        subset = df[df['samples'] == time]\n",
    "        n = dim  # Make sure dim is defined\n",
    "\n",
    "        # Generate column names\n",
    "        column_names = [f'x{i}' for i in range(1, n + 1)]\n",
    "\n",
    "        # Convert each column to tensor and move to device\n",
    "        tensors = [torch.tensor(subset[col].values, dtype=torch.float32).to(device) for col in column_names]\n",
    "\n",
    "        # Stack tensors into 2D tensor\n",
    "        data = torch.stack(tensors, dim=1)\n",
    "        with torch.no_grad():\n",
    "            t = torch.tensor([time], dtype=torch.float32).to(device)\n",
    "            _, g, _, _ = f_net(t, data)\n",
    "        \n",
    "        data_by_time[time] = {'data': subset, 'g_values': g.detach().cpu().numpy()}\n",
    "    \n",
    "    # Combine all g_values\n",
    "    all_g_values = np.concatenate([content['g_values'] for content in data_by_time.values()])\n",
    "    \n",
    "    # Calculate 95th percentile of g_values\n",
    "    vmax_value = np.percentile(all_g_values, 95)\n",
    "    \n",
    "    # Initialize color mapper with clipping\n",
    "    norm = plt.Normalize(vmin=0, vmax=vmax_value, clip=True)\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot data for each time point on same axis\n",
    "    for time, content in data_by_time.items():\n",
    "        subset = content['data']\n",
    "        g_values = content['g_values']\n",
    "        n = dim\n",
    "\n",
    "        column_names = [f'x{i}' for i in range(1, n + 1)]\n",
    "        new_data = subset[column_names]\n",
    "        \n",
    "        if dim_reducer is not None:\n",
    "            data_reduced = dim_reducer.transform(new_data)\n",
    "        else:\n",
    "            data_reduced = new_data.iloc[:, :2].values\n",
    "            \n",
    "        x = adata[df['samples'] == time].obsm['X_umap'][:, 0]#data_reduced[:, 0]\n",
    "        y = adata[df['samples'] == time].obsm['X_umap'][:, 1] #data_reduced[:, 1]\n",
    "        \n",
    "        # Map g_values to colors\n",
    "        colors = plt.cm.rainbow(norm(g_values))\n",
    "        \n",
    "        # Plot scatter with labels for legend\n",
    "        ax.scatter(x, y, c=colors, label=f'Time {time}', alpha=0.7, marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Gene $X_1$')\n",
    "    ax.set_ylabel('Gene $X_2$')\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap='rainbow', norm=norm)\n",
    "    sm.set_array(all_g_values)\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label('Normalized predicted growth rate')\n",
    "    \n",
    "    # Format colorbar ticks\n",
    "    cbar.ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{(x):.2f}'))\n",
    "    \n",
    "    # Save as PDF\n",
    "    plt.savefig(os.path.join(exp_dir, 'g_values_plot.pdf'), format='pdf', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot with f_net and df\n",
    "plot_g_values(df, f_net, dim_reducer=dim_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Assume the following are defined:\n",
    "# - df: DataFrame with columns 'samples', 'x1', 'x2', ..., 'xn'\n",
    "# - dim: Number of dimensions (n)\n",
    "# - device: torch.device (e.g., 'cpu' or 'cpu')\n",
    "# - sf2m_score_model: Your model function that takes t_tensor and data_tensor\n",
    "\n",
    "# Step 1: Extract all time points and data points\n",
    "\n",
    "\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "# Step 2: Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "data_tensor.requires_grad_(True)  # Enable gradient tracking\n",
    "t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Step 3: Compute log density values and gradients\n",
    "log_density_values = sf2m_score_model(t_tensor, data_tensor)\n",
    "log_density_values.backward(torch.ones_like(log_density_values))\n",
    "gradients = data_tensor.grad\n",
    "\n",
    "# Step 4: Move data to CPU for plotting\n",
    "data_np = data_tensor.detach().cpu().numpy()\n",
    "if dim_reducer is not None:\n",
    "    data_np_2d=dim_reducer.transform(data_np)\n",
    "else:\n",
    "    data_np_2d=data_np[:,:2]\n",
    "gradients_np = gradients.detach().cpu().numpy()\n",
    "data_end = data_np + gradients_np\n",
    "if dim_reducer is not None:\n",
    "    data_end_2d=dim_reducer.transform(data_end)\n",
    "else:\n",
    "    data_end_2d=data_end[:,:2]\n",
    "gradients_np=data_end_2d - data_np_2d\n",
    "\n",
    "gradients_np = gradients_np / np.linalg.norm(gradients_np, axis = 1, keepdims=True) * 5\n",
    "times_np = all_times\n",
    "data_np = data_np_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "\n",
    "# Assume your data is already defined\n",
    "# all_data: 50-dimensional data with shape (n_cells, 50)  \n",
    "# gradients: 50-dimensional vector field with shape (n_cells, 50)\n",
    "# pca: Trained PCA or UMAP model for generating X_umap\n",
    "# X_umap: Pre-computed UMAP embeddings with shape (n_cells, 2)\n",
    "\n",
    "# Create AnnData object\n",
    "adata = anndata.AnnData(X=all_data)\n",
    "\n",
    "# Set 'Ms' layer to avoid KeyError: 'Ms'\n",
    "adata.layers['Ms'] = all_data  # Use original 50D data as state matrix\n",
    "# Optional: normalize and log transform (based on data needs)\n",
    "# sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "# sc.pp.log1p(adata)\n",
    "# adata.layers['Ms'] = adata.X.copy()\n",
    "\n",
    "# Set velocity vectors\n",
    "adata.layers['velocity'] = gradients.detach().cpu().numpy()  # Store velocity vectors in layers\n",
    "print(adata.layers['velocity'].shape)  # Confirm shape\n",
    "\n",
    "# Set pre-computed UMAP embeddings\n",
    "if dim_reducer is not None:\n",
    "    X_umap = dim_reducer.transform(all_data)  # Assume pca is trained dim reduction model\n",
    "else:\n",
    "    X_umap = all_data[:,:2]\n",
    "adata.obsm['X_umap'] = X_umap\n",
    "print(adata.obsm['X_umap'].shape)  # Confirm UMAP embedding shape\n",
    "adata.obs['time'] = all_times\n",
    "if adata.layers['velocity'].shape[1] != 2:\n",
    "    # Compute neighbor graph (required for velocity graph)\n",
    "    sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')  # Calculate neighbors based on high-dim data\n",
    "\n",
    "    # Compute velocity graph\n",
    "    scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=16)  # Build velocity graph from high-dim velocity vectors\n",
    "\n",
    "    # Project velocities to UMAP space\n",
    "    scv.tl.velocity_embedding(adata, basis='umap', vkey='velocity')  # Project velocities to UMAP\n",
    "else:\n",
    "    adata.obsm['velocity_umap'] = adata.layers['velocity']\n",
    "\n",
    "# Plot\n",
    "adata.obs['time_categorical'] = pd.Categorical(adata.obs['time'])\n",
    "\n",
    "# Visualization settings\n",
    "scv.settings.set_figure_params('scvelo')  # Set scvelo plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.pl.velocity_embedding_stream(\n",
    "    adata,\n",
    "    basis='umap',\n",
    "    color='time_categorical',\n",
    "    figsize=(7, 5),\n",
    "    density=3,\n",
    "    title='Score Stream Plot',\n",
    "    legend_loc='right',\n",
    "    palette='viridis',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "device = 'cpu'\n",
    "f_net.to(device)\n",
    "sf2m_score_model.to(device)\n",
    "# Assuming df, dim, device, f_net are defined\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Calculate gradients\n",
    "with torch.no_grad():  # No need to track gradients to save memory\n",
    "    gradients = f_net.v_net(t_tensor, data_tensor)\n",
    "\n",
    "# Convert to NumPy array\n",
    "gradients_np = gradients.cpu().numpy()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Assume the following are defined:\n",
    "# - df: DataFrame with columns 'samples', 'x1', 'x2', ..., 'xn'\n",
    "# - dim: Number of dimensions (n)\n",
    "# - device: torch.device (e.g., 'cpu' or 'cpu')\n",
    "# - sf2m_score_model: Your model function that takes t_tensor and data_tensor\n",
    "\n",
    "# Step 1: Extract all time points and data points\n",
    "all_times = df['samples'].values\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "#t_value = 4.0  \n",
    "#t_tensor = torch.tensor([t_value] * all_data.shape[0]).unsqueeze(1).float().to(device)\n",
    "t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Step 2: Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(all_data, dtype=torch.float32).to(device)\n",
    "data_tensor.requires_grad_(True)  # Enable gradient tracking\n",
    "#t_tensor = torch.tensor(all_times, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Step 3: Compute log density values and gradients\n",
    "log_density_values = sf2m_score_model(t_tensor, data_tensor)\n",
    "log_density_values.backward(torch.ones_like(log_density_values))\n",
    "gradients_score = data_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall=gradients_np+gradients_score.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "all_data = df[[f'x{i}' for i in range(1, dim + 1)]].values\n",
    "\n",
    "# Assume data is defined: all_data, gradients, X_umap, all_times, cell_type\n",
    "adata = anndata.AnnData(X=all_data)\n",
    "adata.layers['Ms'] = all_data\n",
    "adata.layers['velocity'] = overall\n",
    "adata.obsm['X_umap'] = data_np\n",
    "adata.obs['time'] = all_times\n",
    "velocity_norm = adata.layers['velocity'] \n",
    "# Verify shapes\n",
    "print(\"Velocity shape:\", adata.layers['velocity'].shape)\n",
    "print(\"UMAP shape:\", adata.obsm['X_umap'].shape)\n",
    "\n",
    "# Compute neighborhood graph\n",
    "#sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')\n",
    "\n",
    "# Compute velocity graph\n",
    "#scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=1)\n",
    "#\n",
    "# Handle velocity projection\n",
    "if adata.layers['velocity'].shape[1] != 2:\n",
    "    sc.pp.neighbors(adata, n_neighbors=30, use_rep='X')\n",
    "    scv.tl.velocity_graph(adata, vkey='velocity', n_jobs=16)\n",
    "    scv.tl.velocity_embedding(adata, basis='umap', vkey='velocity')\n",
    "else:\n",
    "    adata.obsm['velocity_umap'] = adata.layers['velocity']\n",
    "\n",
    "# Plot\n",
    "\n",
    "adata.obs['time_categorical'] = pd.Categorical(adata.obs['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.settings.set_figure_params('scvelo')\n",
    "scv.pl.velocity_embedding_stream(\n",
    "    adata,\n",
    "    basis='umap',\n",
    "    color='time_categorical',\n",
    "    figsize=(7, 5),\n",
    "    density=1,\n",
    "    title='All velocity Stream Plot',\n",
    "    legend_loc='right',\n",
    "    palette='viridis',\n",
    "    save=exp_dir+'/all_velocity_stream_plot.svg'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepRUOT.utils import euler_sdeint\n",
    "import random\n",
    "n_times = all_times.max() + 1\n",
    "data=torch.tensor(df[df['samples']==0].values,dtype=torch.float32).requires_grad_()\n",
    "data_t0 = data[:, 1:].to(device).requires_grad_()\n",
    "print(data_t0.shape)\n",
    "x0=data_t0.to(device)\n",
    "\n",
    "dim_reducer = joblib.load(os.path.join(exp_dir, 'dim_reduction.pkl'))\n",
    "\n",
    "class SDE(torch.nn.Module):\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"ito\"\n",
    "\n",
    "    def __init__(self, ode_drift, g, score, input_size=(3, 32, 32), sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.drift = ode_drift\n",
    "        self.score = score\n",
    "        self.input_size = input_size\n",
    "        self.sigma = sigma\n",
    "        self.g_net = g\n",
    "\n",
    "    # Drift\n",
    "    def f(self, t, y):\n",
    "        z, lnw = y\n",
    "        drift=self.drift(t, z)\n",
    "        dlnw = self.g_net(t, z)\n",
    "        num = z.shape[0]\n",
    "        t = t.expand(num, 1)  # Keep gradient information of t and expand its shape\n",
    "        return (drift+self.score.compute_gradient(t, z), dlnw)\n",
    "\n",
    "    # Diffusion\n",
    "    def g(self, t, y):\n",
    "        return torch.ones_like(y)*self.sigma\n",
    "    \n",
    "x0_subset = x0.to(device)\n",
    "\n",
    "x0_subset = x0_subset.to(device)\n",
    "lnw0 = torch.log(torch.ones(x0_subset.shape[0], 1) / x0_subset.shape[0]).to(device)\n",
    "initial_state = (x0_subset, lnw0)\n",
    "\n",
    "# Define SDE object\n",
    "sde = SDE(f_net.v_net, \n",
    "          f_net.g_net, \n",
    "          sf2m_score_model, \n",
    "          input_size=(2,), \n",
    "          sigma=config['score_train']['sigma'])\n",
    "\n",
    "# Define time points, assuming total 200 integration steps\n",
    "ts = torch.linspace(0, n_times - 1, 100, device=device)\n",
    "\n",
    "# Manual SDE integration\n",
    "sde_traj, traj_lnw = euler_sdeint(sde, initial_state, dt=0.1, ts=ts)\n",
    "# Transfer to CPU if needed:\n",
    "sde_traj, traj_lnw = sde_traj.cpu(), traj_lnw.cpu()\n",
    "\n",
    "\n",
    "sample_number = 100  # For example, sample 10\n",
    "sample_indices = random.sample(range(sde_traj.size(1)), sample_number)\n",
    "sampled_sde_trajec = sde_traj[:, sample_indices, :]\n",
    "sampled_sde_trajec.shape\n",
    "sampled_sde_trajec = sampled_sde_trajec.tolist()\n",
    "sampled_sde_trajec = np.array(sampled_sde_trajec, dtype=object)\n",
    "np.save(exp_dir+'/sde_trajec.npy', sampled_sde_trajec)\n",
    "\n",
    "\n",
    "ts_points = torch.tensor(sorted(df.samples.unique()), dtype=torch.float32)\n",
    "print(ts_points)\n",
    "\n",
    "sde_point, traj_lnw = euler_sdeint(sde, initial_state, dt=0.1, ts=ts_points)\n",
    "print(sde_point.shape)\n",
    "print(traj_lnw.shape)\n",
    "weight = torch.exp(traj_lnw)\n",
    "weight_normed = weight/weight.sum(dim = 1, keepdim = True)\n",
    "\n",
    "sde_point_np = sde_point.detach().cpu().numpy()\n",
    "sde_point_list = sde_point_np.tolist()\n",
    "sde_point_array = np.array(sde_point_list, dtype=object)\n",
    "np.save(exp_dir+'/sde_point.npy', sde_point_array)\n",
    "np.save(exp_dir+'/sde_weight.npy', weight_normed.detach().cpu().numpy())\n",
    "\n",
    "from DeepRUOT.plots import new_plot_comparisions2\n",
    "sde_point = np.load(exp_dir+'/sde_point.npy', allow_pickle=True)\n",
    "sde_point.shape\n",
    "\n",
    "sde_trajec = np.load(exp_dir+'/sde_trajec.npy', allow_pickle=True)\n",
    "samples = df.iloc[:, 0]  # Get samples column\n",
    "features = df.iloc[:, 1:]  # Get feature columns\n",
    "\n",
    "# Transform data based on whether dim_reducer is provided\n",
    "if dim_reducer is not None:\n",
    "    # Transform original data\n",
    "    reduced_features = dim_reducer.transform(features)\n",
    "    \n",
    "    # Transform generated points\n",
    "    generated_flattened = sde_point.reshape(-1, features.shape[1])\n",
    "    generated_reduced = dim_reducer.transform(generated_flattened)\n",
    "    generated_reduced = generated_reduced.reshape(sde_point.shape[0], -1, 2)\n",
    "    \n",
    "    # Transform trajectories \n",
    "    trajectories_flattened = sde_trajec.reshape(-1, features.shape[1])\n",
    "    trajectories_reduced = dim_reducer.transform(trajectories_flattened)\n",
    "    trajectories_reduced = trajectories_reduced.reshape(sde_trajec.shape[0], -1, 2)\n",
    "else:\n",
    "    # Use first two dimensions\n",
    "    reduced_features = features.iloc[:,:2].values\n",
    "    generated_reduced = sde_point[:,:,:2] \n",
    "    trajectories_reduced = sde_trajec[:,:,:2]\n",
    "\n",
    "# Create dataframe with reduced dimensions\n",
    "new_df = pd.DataFrame(reduced_features, columns=['x1', 'x2'])\n",
    "new_df['samples'] = samples\n",
    "\n",
    "new_plot_comparisions2(\n",
    "    new_df, generated_reduced, trajectories_reduced,\n",
    "    palette='viridis', df_time_key='samples', \n",
    "    save=True, path=exp_dir, file='sde_trajectories.pdf',\n",
    "    x='x1', y='x2', z='x3', is_3d=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "meta_data = sc.read_h5ad('/lustre/home/2100011778/DeepRUOTv2_test_data/test_data/WO_FG_D10_D20_SCUD_tumor_cca_downsample_0120_20dims.h5ad')\n",
    "celltype = meta_data.obs['celltype']\n",
    "\n",
    "data=torch.tensor(df[df['samples'] == 0].values,dtype=torch.float32).requires_grad_()\n",
    "data_t0 = data[:, 1:].to(device).requires_grad_()\n",
    "print(data_t0.shape)\n",
    "x0=data_t0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data['celltype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 CSV 文件\n",
    "df_new = pd.read_csv(os.path.join(DATA_DIR, config['data']['file_path']))\n",
    "all_labels = meta_data.obs['celltype'].values\n",
    "\n",
    "# 添加 Annotation 列\n",
    "df_new['Annotation'] = all_labels\n",
    "df_new\n",
    "# # 获取 Annotation 的类别和颜色\n",
    "# if pd.api.types.is_categorical_dtype(adata.obs['Annotation']):\n",
    "#     categories = adata.obs['Annotation'].cat.categories\n",
    "# else:\n",
    "#     categories = adata.obs['Annotation'].unique()  # 如果不是 categorical 类型\n",
    "\n",
    "# df_new.to_csv(os.path.join(DATA_DIR, 'test_data_0710_pro_HB_with_annotation.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 假设数据已经加载到 DataFrame 中\n",
    "# df = pd.read_csv('your_data.csv')  # 替换为你的数据文件路径\n",
    "\n",
    "# 步骤 1：准备数据\n",
    "# 提取特征和标签\n",
    "X = df_new.iloc[:,:-1].values\n",
    "y = df_new['Annotation'].values\n",
    "\n",
    "# 对标签进行编码\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(y_encoded[:100])\n",
    "# 划分训练集和测试集（80% 训练，20% 测试）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 步骤 2：构建 MLP 模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 第一隐藏层\n",
    "        self.relu = nn.LeakyReLU()                          # 激活函数\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # 第二隐藏层\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes) # 输出层\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# 设置模型参数\n",
    "input_size = 21  # 特征数量\n",
    "hidden_size = 128 # 隐藏层神经元数量\n",
    "num_classes = len(label_encoder.classes_)  # 类别数量\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "\n",
    "# 步骤 3：训练模型\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "num_epochs = 10000\n",
    "model = model.cuda()\n",
    "X_train = X_train.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置为训练模式\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.cuda())\n",
    "    \n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "    loss.backward()        # 反向传播\n",
    "    optimizer.step()       # 更新参数\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        # 步骤 4：评估模型\n",
    "        model.eval()  # 设置为评估模式\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test.cuda())\n",
    "            _, predicted = torch.max(outputs, 1)  # 获取预测类别\n",
    "            accuracy = accuracy_score(y_test, predicted.cpu())\n",
    "            print(f'测试集准确率: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), exp_dir + '/mlp_classifier.pth')\n",
    "print(\"模型已保存到 'mlp_classifier.pth'\")\n",
    "model.eval()\n",
    "\n",
    "# 初始化存储分类结果和颜色的列表\n",
    "predicted_labels_list = []\n",
    "# predicted_colors_list = []\n",
    "\n",
    "ts = [0.0, 1.0, 2.0, 3.0]\n",
    "# 对每个时间点进行分类\n",
    "predicted_labels_list.append(df_new[df_new['samples']==0]['Annotation'].values)\n",
    "\n",
    "for i in range(1, len(sde_point)):\n",
    "    t = ts[i]  # 当前时间点\n",
    "    traj_t = np.array(sde_point[i], dtype = np.float64)  # 当前时间点的特征，形状 (n, 12)\n",
    "    traj_t = torch.tensor(traj_t)\n",
    "    n_samples = traj_t.shape[0]\n",
    "    \n",
    "    # 构造输入：samples (时间) + 特征\n",
    "    samples_t = t * torch.ones((n_samples, 1))  # (n, 1)\n",
    "    input_t = torch.cat((samples_t, traj_t), dim=1)  # (n, 13)\n",
    "    \n",
    "    # 使用模型预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_t.float().cuda())\n",
    "        _, predicted = torch.max(outputs, 1)  # 预测类别索引\n",
    "        predicted_labels = label_encoder.inverse_transform(predicted.detach().cpu().numpy())  # 转换为原始标签\n",
    "    \n",
    "    # 获取颜色\n",
    "    # predicted_colors = [label_to_color[label] for label in predicted_labels]\n",
    "    \n",
    "    # 存储结果\n",
    "    predicted_labels_list.append(predicted_labels)\n",
    "    # predicted_colors_list.append(predicted_colors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import importlib\n",
    "import nbformat\n",
    "importlib.reload(nbformat)\n",
    "\n",
    "\n",
    "\n",
    "# 第一步：统计细胞从一个时间点到下一个时间点的转变\n",
    "links = []\n",
    "num_timepoints = len(predicted_labels_list)\n",
    "\n",
    "for t in range(num_timepoints - 1):\n",
    "    # 使用 pandas 快速统计\n",
    "    df = pd.DataFrame({\n",
    "        'source': predicted_labels_list[t],\n",
    "        'target': predicted_labels_list[t+1]\n",
    "    })\n",
    "    \n",
    "    # 统计每种转变的数量\n",
    "    counts = df.groupby(['source', 'target']).size().reset_index(name='value')\n",
    "    \n",
    "    # 为节点名称添加时间后缀，以区別不同时间的节点\n",
    "    counts['source'] = counts['source'].astype(str) + f'_T{t+1}'\n",
    "    counts['target'] = counts['target'].astype(str) + f'_T{t+2}'\n",
    "    \n",
    "    links.append(counts)\n",
    "\n",
    "# 将所有时间点的连接合并成一个 DataFrame\n",
    "all_links_df = pd.concat(links, axis=0)\n",
    "\n",
    "\n",
    "# 第二步：构建绘图所需的数据结构\n",
    "# 获取所有不重复的节点\n",
    "all_nodes = pd.unique(all_links_df[['source', 'target']].values.ravel('K'))\n",
    "\n",
    "# 创建一个从节点名称到索引的映射\n",
    "node_indices = {node: i for i, node in enumerate(all_nodes)}\n",
    "\n",
    "# 将 DataFrame 中的 source 和 target 替换为对应的索引\n",
    "all_links_df['source_idx'] = all_links_df['source'].map(node_indices)\n",
    "all_links_df['target_idx'] = all_links_df['target'].map(node_indices)\n",
    "\n",
    "# 为不同细胞类型定义颜色\n",
    "# 提取基础细胞类型名（去掉_T*后缀）\n",
    "base_types = sorted(list(set([node.split('_')[0] for node in all_nodes])))\n",
    "color_palette = ['#440154', '#3b528b', '#21908d', '#5dc863', '#fde725'] # 可自定义颜色\n",
    "color_map = {base_type: color_palette[i % len(color_palette)] for i, base_type in enumerate(base_types)}\n",
    "node_colors = [color_map[node.split('_')[0]] for node in all_nodes]\n",
    "\n",
    "\n",
    "# 第三步：使用 Plotly 绘图\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    # --- 节点定义 ---\n",
    "    node=dict(\n",
    "      pad=25,\n",
    "      thickness=20,\n",
    "      line=dict(color=\"black\", width=0.5),\n",
    "      label=all_nodes,\n",
    "      color=node_colors,\n",
    "      # 核心：手动指定 x 坐标来按时间列对齐\n",
    "      x=[int(node.split('_T')[1]) / (num_timepoints + 1) for node in all_nodes],\n",
    "      # y 坐标可由plotly自动确定，也可手动微调\n",
    "      # y=[...] \n",
    "    ),\n",
    "    # --- 连接定义 ---\n",
    "    link=dict(\n",
    "      source=all_links_df['source_idx'],\n",
    "      target=all_links_df['target_idx'],\n",
    "      value=all_links_df['value'],\n",
    "      # 可以为连接也设置颜色\n",
    "      # color = ...\n",
    "  ))])\n",
    "\n",
    "# 更新图表标题和布局\n",
    "fig.update_layout(\n",
    "    title_text=\"细胞谱系演变桑基图\",\n",
    "    font_family=\"Arial\",\n",
    "    font_size=12,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --------------------\n",
    "\n",
    "\n",
    "# ####################################################################\n",
    "# ### 主要改动：设定目标细胞类型并筛选数据 ###\n",
    "# ####################################################################\n",
    "\n",
    "target_types = ['SCUD']\n",
    "\n",
    "# 1. 对每个时间点，检查哪些细胞的类型在 `target_types` 列表中\n",
    "#    np.isin 会返回一个布尔数组 (True/False mask)\n",
    "masks = [np.isin(labels, target_types) for labels in predicted_labels_list]\n",
    "\n",
    "# 2. 合并所有时间点的掩码。只要一个细胞在任何时间点出现过目标类型，\n",
    "#    我们就保留它。我们使用逻辑“或”操作来合并。\n",
    "#    np.logical_or.reduce 会对列表中的所有数组进行逐元素的“或”运算。\n",
    "combined_mask = np.logical_or.reduce(masks)\n",
    "\n",
    "# 3. 根据最终的合并掩码，获取所有相关细胞的索引\n",
    "target_indices = np.where(combined_mask)[0]\n",
    "\n",
    "# 4. 使用这些索引筛选出完整的谱系数据\n",
    "if len(target_indices) > 0:\n",
    "    filtered_predicted_label_list = [labels[target_indices] for labels in predicted_labels_list]\n",
    "else:\n",
    "    filtered_predicted_label_list = [np.array([]) for _ in predicted_labels_list]\n",
    "    print(f\"警告：在任何时间点都未找到细胞类型在 {target_types} 中的谱系。将生成一个空图。\")\n",
    "    \n",
    "# ####################################################################\n",
    "# ### 后续代码与之前完全相同 ###\n",
    "# ####################################################################\n",
    "\n",
    "# 第一步：统计转变\n",
    "links = []\n",
    "num_timepoints = len(filtered_predicted_label_list)\n",
    "if num_timepoints > 1 and len(filtered_predicted_label_list[0]) > 0:\n",
    "    for t in range(num_timepoints - 1):\n",
    "        df = pd.DataFrame({'source': filtered_predicted_label_list[t], 'target': filtered_predicted_label_list[t+1]})\n",
    "        counts = df.groupby(['source', 'target']).size().reset_index(name='value')\n",
    "        counts['source'] = counts['source'].astype(str) + f'_T{t+1}'\n",
    "        counts['target'] = counts['target'].astype(str) + f'_T{t+2}'\n",
    "        links.append(counts)\n",
    "    all_links_df = pd.concat(links, axis=0)\n",
    "\n",
    "    # 第二步：构建绘图数据\n",
    "    all_nodes = pd.unique(all_links_df[['source', 'target']].values.ravel('K'))\n",
    "    node_indices = {node: i for i, node in enumerate(all_nodes)}\n",
    "    all_links_df['source_idx'] = all_links_df['source'].map(node_indices)\n",
    "    all_links_df['target_idx'] = all_links_df['target'].map(node_indices)\n",
    "    base_types = sorted(list(set([node.split('_')[0] for node in all_nodes])))\n",
    "    color_palette = ['#440154', '#3b528b', '#21908d', '#5dc863', '#fde725']\n",
    "    color_map = {base_type: color_palette[i % len(color_palette)] for i, base_type in enumerate(base_types)}\n",
    "    node_colors = [color_map[node.split('_')[0]] for node in all_nodes]\n",
    "\n",
    "    # 第三步：绘图\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "          pad=25, thickness=20, line=dict(color=\"black\", width=0.5),\n",
    "          label=all_nodes, color=node_colors,\n",
    "          x=[int(node.split('_T')[1]) / (num_timepoints + 1) for node in all_nodes],\n",
    "        ),\n",
    "        link=dict(\n",
    "          source=all_links_df['source_idx'], target=all_links_df['target_idx'],\n",
    "          value=all_links_df['value'],\n",
    "      ))])\n",
    "    fig.update_layout(title_text=f\"包含 {target_types} 的细胞谱系图\", font_family=\"Arial\", font_size=12, plot_bgcolor='white')\n",
    "else:\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(title_text=f\"未找到包含 {target_types} 的细胞谱系\", annotations=[dict(text=\"无数据显示\", xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=20))])\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepRUOTv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
